{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHiWpBcgUKgS",
        "outputId": "a1ef09b4-d531-4975-f523-53fe8627e762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.54.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.34.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Prompt:\n",
            " The impact of artificial intelligence on education is\n",
            "\n",
            "üîπ Generated Text:\n",
            "\n",
            "The impact of artificial intelligence on education is clear.\n",
            "\"The most important thing is that we have a system that is able to predict what is going to happen in the future,\" said Dr.\n",
            "Michael S.\n",
            "Karp, a professor of psychology at the University of California, San Francisco.\n",
            "\"We have to be able, in a way, to anticipate what will happen.\"\n",
            ".\n",
            ".\n",
            ".\n",
            ", which is a combination of the two.\n",
            "The first is the ability to make predictions about what the world will look like in 20 years.\n",
            "This is called the \"superintelligence\" hypothesis.\n",
            "It is based on the idea that the human mind is capable of making predictions based upon the information it receives from the environment.\n",
            "In other words, it can\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# üõ†Ô∏è Install Dependencies\n",
        "# =========================\n",
        "!pip install transformers\n",
        "\n",
        "# =========================\n",
        "# üì¶ Import Libraries\n",
        "# =========================\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import re\n",
        "\n",
        "# =========================\n",
        "# ü§ñ Load GPT-2 Model and Tokenizer\n",
        "# =========================\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "# =========================\n",
        "# üß† Text Generation Function\n",
        "# =========================\n",
        "def generate_text(prompt, max_length=150, temperature=0.8, top_k=50):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(\n",
        "        inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        early_stopping=True\n",
        "    )\n",
        "\n",
        "    raw_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Add newlines after sentence-ending punctuation\n",
        "    formatted_text = re.sub(r'(?<=[.!?])\\s+', '\\n', raw_text)\n",
        "\n",
        "    return formatted_text\n",
        "\n",
        "# =========================\n",
        "# üìù Example Usage\n",
        "# =========================\n",
        "prompt = \"The impact of artificial intelligence on education is\"\n",
        "print(\"üîπ Prompt:\\n\", prompt)\n",
        "\n",
        "generated_text = generate_text(prompt)\n",
        "print(\"\\nüîπ Generated Text:\\n\")\n",
        "print(generated_text)\n"
      ]
    }
  ]
}